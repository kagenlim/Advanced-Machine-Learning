{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0uz_2CEH9yU"
   },
   "source": [
    "This notebook serves as an outline and summary of the three projects I have completed, utilizing machine learning and deep learning models, for one of my Master's courses at Columbia University, Projects in Advanced Machine Learning (QMSS-GR5054). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNrjD42qXcHX"
   },
   "source": [
    "## Overview of Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMDI4qFERxFR"
   },
   "source": [
    "The three projects I have completed for this course span multiple *types* of data. While analysis of structured, tabular data (i.e., the kind you would expect to see in spreadsheets) remains critically important, there are vast opportunities in the vast amount of **unstructured** data that exist online, including **image data** and **textual data**. In line with these growing needs, my course instructors intentionally introduced us to the use of machine learning models and neural networks to work with these data types. We focused on supervised learning problems for this course (i.e., generalizing from some training data, with evaluation metrics on some held-out test data). \n",
    "\n",
    "This table here summarizes the projects that I have worked on; you are most welcome to click on this links here that will take you directly to the relevant notebook for the respective project:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpHDFB78Md-o"
   },
   "source": [
    "| Type of Data | Title of Project | Link to ipynb |\n",
    "|-|-|-|\n",
    "| Tabular | Predicting Happiness from U.N. World Happiness Data | https://github.com/kagenlim/Advanced-Machine-Learning/blob/main/Tabular_Data_UN/UN%20Tabular%20Dataset_KagenNB.ipynb |\n",
    "| Image (X-Rays) | Predicting COVID-19 Positivity from X-Ray Data | https://github.com/kagenlim/Advanced-Machine-Learning/blob/main/Image_Data_X-Ray/COVID_X-Ray_Dataset_KagenNB.ipynb |\n",
    "| Text (Twitter) | Predicting COVID-19 Related Misinformation from Twitter Data | https://github.com/kagenlim/Advanced-Machine-Learning/blob/main/Text_Data_Misinformation/Text_Data_COVID_Misinformation_KagenNB.ipynb |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5elUu8yCXMtw"
   },
   "source": [
    "I include more information in the sections below, about these projects, with the hope that this would make this material more accessible. Please do reach out, through [email](mailto:kagen.lim@columbia.edu) or [LinkedIn](https://www.linkedin.com/in/kagen-lim/) if you might like to have a conversation about these projects, or if there is any way that I might be able to help!\n",
    "\n",
    "-Kagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZNuG_juXur_"
   },
   "source": [
    "## Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5HO4QGsK7f_"
   },
   "source": [
    "[In this notebook](https://github.com/kagenlim/Advanced-Machine-Learning/blob/main/Tabular_Data_UN/UN%20Tabular%20Dataset_KagenNB.ipynb), I explored the [2019 United Nations World Happiness Dataset](https://github.com/kagenlim/Advanced-Machine-Learning/blob/main/Tabular_Data_UN/worldhappiness2019_onenewvariable_final.xlsx). \n",
    "\n",
    "The goal here was to predict `Happiness_level`, based on some given features that are indicators for how life is like in that country (e.g., predictors like `GDP per capita` and `Social support`. `Happiness_level` was an ordinal variable with five levels, which made this a multi-class classification problem.\n",
    "\n",
    "This was a rather small dataset, **both** in terms of the number of features (8 were usable as predictors) and with regards to the sample size (n=156). With a typical `train_test_split` approach for supervised learning, this meant that the training and test sets were even smaller. I used a Random Forest Classifier to perform automatic feature selection, and based on this I removed one feature from the set of predictors I had, leaving me with 7 predictor variables. This made the application of machine learning admittedly very challenging, since the size and dimensionality of training data usually impacts model performance quite greatly. \n",
    "\n",
    "I employed three Classifier models to predict happiness levels -- a Random Forest Classifier, a Gradient Boosted Classifier and a fully-connected Neural Network (multilayer perceptron). Hyperparameter tuning was conducted with `GridSearchCV`. Upon model evaluation on the held out test set, the fully-connected Neural Network performed the best on this data; but this level of performance was admittedly not great, since it was predicting at chance level (accuracy and f-1 score = 0.50). \n",
    "\n",
    "I would expect this model to perform a lot better if more data was merged in, but I would say that this was also a big point of learning for me, early in the course -- machine learning is as much about the quality of the data as it is about building great models around that data. \n",
    "\n",
    "Here is the structure of my best model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GPjasr1cLSD8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) #ignore tensorflow warnings\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout,BatchNormalization\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EHbEPEsez_aY"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(80, input_dim= 7, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))) #7 predictors going in \n",
    "model.add(Dense(80, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.add(Dense(80, activation='relu', kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.add(Dense(5, activation='softmax')) # 5 classes; multi-class classification problem\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsYHXgaELMbk",
    "outputId": "089736ef-0d88-4b9d-9d36-e00ca86644fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 80)                640       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 405       \n",
      "=================================================================\n",
      "Total params: 14,005\n",
      "Trainable params: 14,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUtCj6xFXwbL"
   },
   "source": [
    "## Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQGfep5IPYV_"
   },
   "source": [
    "Moving on from tabular data, is when the content of the course became very exciting to me, since I had not worked with non-tabular data prior to my time at Columbia. (I really do enjoy working with machine learning on tabular data though -- please talk to me about my M.A. thesis!) This was also the point that I learnt about transfer learning approaches, and how I could build great models based on the amazing work that has already been done by the wider machine learning community. \n",
    "\n",
    "For image data, the model of choice, given enough data, is usually some variant of a Convolutional Neural Network. These models are adept at parameter sharing due to the function of the convolutional layers (also known as filters), which can pick up on common patterns in images, no matter where these patterns might be places since these filters and convolved across the entire image.\n",
    "\n",
    "[In this notebook](https://github.com/kagenlim/Advanced-Machine-Learning/blob/main/Image_Data_X-Ray/COVID_X-Ray_Dataset_KagenNB.ipynb), I worked with a dataset of X-Ray images that was originally used for this pre-print article: \n",
    "\n",
    "```\n",
    "M.E.H. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M.A. Kadir, Z.B. Mahbub, K.R. Islam, M.S. Khan, A. Iqbal, N. Al-Emadi, M.B.I. Reaz, “Can AI help in screening Viral and COVID-19 pneumonia?” arXiv preprint, 29 March 2020, https://arxiv.org/abs/2003.13145 (Links to an external site.)\n",
    "```\n",
    "This was quite a substantial dataset, with 3886 X-Ray images. \n",
    "\n",
    "The goal here was to build some models to correctly classify X-Ray images into three classes: 1) COVID pneumonia patient, 2) Non-COVID viral pneumonia patient and 3) Healthy patient. This was a particularly apt dataset for convolutional neural networks to address, because there were indeed qualitative differences in the images -- COVID patients often have a particularly blurry X-Ray scan, also known as [ground glass opacity](https://pubs.rsna.org/doi/full/10.1148/ryct.2020200280). Assuming that chest scans are done for a number of patients, a good classifier to tackle this problem would be a particularly meaningful diagnostic tool for medical professionals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-Bv6fn5Ulgh"
   },
   "source": [
    "I tried three models; for two of these models, I treated this as a transfer learning problem, and relied on the available pre-trained weights from the deployment of these models on the ImageNet competition (this is a huge Image Classification competition in the machine learning/computer vision community, which involves building large models that can accurately classify up to 1000 classes, with over a million training images. The transfer learning models I used were: the [VGG16](https://arxiv.org/pdf/1409.1556.pdf) and [InceptionV3](https://arxiv.org/pdf/1512.00567v3.pdf) Models. My third model was a [SqueezeNet Model](https://arxiv.org/pdf/1602.07360.pdf), but I did not use pre-trained weights for this.\n",
    "\n",
    "On hindsight, the use of InceptionV3 may not have been ideal, because this an extremely deep network, that would not have been effective for the dataset size I was working with. SqueezeNet, however, had an impressive showing with a f-1 score of 0.952.\n",
    "\n",
    "My best performing model was the VGG16 model, which performed at a f-1 score of 0.986. This was the structure of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9B0kFAJe0AD8",
    "outputId": "1b51245e-c1a1-40ef-c4c6-8374f8cf703b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 192, 192, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 192, 192, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 192, 192, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 96, 96, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 96, 96, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 96, 96, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 48, 48, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 48, 48, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 48, 48, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 48, 48, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 24, 24, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 24, 24, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 24, 24, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18432)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              18875392  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 33,593,155\n",
      "Trainable params: 18,878,467\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16 \n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "base_model = VGG16(input_shape=(192,192,3),\n",
    "                                               include_top=False, # Drops fully connected layers and output layer structure from pretrained model. #original had > 1000 categories#\n",
    "                                               weights='imagenet') # Using weight from model trained to 1000 categories of imagenet competition \n",
    "\n",
    "base_model.trainable = False \n",
    "\n",
    "flat1 = Flatten()(base_model.layers[-1].output) #plugging in first half to the second half; from transfer learning model\n",
    "class1 = Dense(1024, activation='relu')(flat1) #stack below\n",
    "output = Dense(3, activation='softmax')(class1) #softmax layer to end off \n",
    "# define new model\n",
    "model2 = Model(inputs=base_model.inputs, outputs=output)\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qO9MmpckXybk"
   },
   "source": [
    "## Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMKciAvVc877"
   },
   "source": [
    "Moving on to textual data was also a fascinating experience -- this was when I began exploring recurrent neural networks, which are important for text models where sequence and context can make a huge difference with regard to the model's performance. This is also when I learnt about word embeddings, including pre-trained word vectors like those from [GloVe](https://nlp.stanford.edu/projects/glove/), which do also assist in adding contextual inputs for a model's processing of text. \n",
    "\n",
    "[In this notebook](https://github.com/kagenlim/Advanced-Machine-Learning/blob/main/Text_Data_Misinformation/Text_Data_COVID_Misinformation_KagenNB.ipynb), I worked with a dataset of Tweets, that were each labelled as either 'real' (i.e., authentic information) or 'fake' (i.e., false information). This was a substantial dataset, with 8560 tweets in total. The dataset originated from this publication:\n",
    "\n",
    "```\n",
    "Shahi, G. K., Dirkson, A. and Majchrzak, T.A. \"An exploratory study of Covid-19 misinformation on twitter.\" Online Social Networks and Media 22 (2021): 100104.\n",
    "```\n",
    "\n",
    "The goal here was to build some models that could accurately classify the Tweets into these two categories, making this a binary classification problem. The application of this model would be fairly straightforward -- social media companies and governments, who deal with a great deal of textual data and misinformation at a large scale, might be able to benefit from such models that automatically classify Tweets as misinformation or genuine. \n",
    "\n",
    "I fit four models on this data. All of these models started with an embedding layer:\n",
    "\n",
    "> Model 1: One Embedding Layer --> Stacked LSTM Layers (Two Layers)\n",
    "\n",
    "> Model 2: One Embedding Layer --> Stacked LSTM Layers (Two Layers, with Dropout Regularization on the second one)\n",
    "\n",
    "> Model 3: One Embedding Layer  --> One Conv1D Layer --> Stacked LSTM Layers (Two Layers, with Dropout Regularization on the second one)\n",
    "\n",
    ">Model 4: One Embedding Layer --> Stacked LSTM Layers (Two Layers, with the first one as a Bidirectional LSTM Layer and the second one as a LSTM Layer with Dropout Regularization)\n",
    "\n",
    "Each of these models performed reasonably well, with f-1 scores of 0.94 or above when each of these models were evaluated on held-out test data. The best performing model was Model 4, which had a bidirectional LSTM layer. This was an interesting finding, given that bidirectional layers make a lot of sense of textual data. Such layers once again address the issue of contextual information for textual processing; just like how human readers would have access to the context slightly before a word and slightly after a word, bidirectional layers enable information from before a given word and after a given word to be fed into the model concurrently. Given a bit more time, I would have experimented with GloVe embeddings and how they might enhance the model's performance.\n",
    "\n",
    "This was the structure of my best model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dccLDS2x0BO6",
    "outputId": "f5eb376a-c8d8-460a-c7a2-545654a0f835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 40, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 40, 80)            45120     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 60)                33840     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 40)                2440      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 82        \n",
      "=================================================================\n",
      "Total params: 1,081,482\n",
      "Trainable params: 1,081,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 100, input_length=40))\n",
    "model.add(Bidirectional(LSTM(40, activation='tanh', return_sequences=True)))\n",
    "model.add(LSTM(60, dropout=0.2, recurrent_dropout=0.2, activation='tanh'))\n",
    "model.add(Dense(40, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Portfolio_Overview.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
